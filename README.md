# Kolosis: Cognitive Multi-Stream Architecture

> **âš ï¸ Research Preview**: Core architecture validated on WikiText-103. Full benchmarking in progress.

Kolosis is a novel neural architecture that separates language processing into specialized **cognitive streams** (Symbol, Concept, Semantic, Temporal), inspired by human cognition. Unlike traditional transformers that use a single unified representation, Kolosis enables interpretable, controllable, and efficient language modeling.

## ğŸ¯ Key Features

- **ğŸ§  Cognitive Specialization**: Separate streams for different types of thinking
- **ğŸ“Š Interpretable**: See which cognitive functions are active via fusion weights
- **âš¡ Efficient**: Achieves competitive performance with 56% fewer parameters
- **ğŸ›ï¸ Controllable**: Tune cognitive balance for different tasks
- **ğŸ” Transparent**: Full visibility into model reasoning

## ğŸ“ˆ Current Results

### Kolosis V2 Minimal (27.4M parameters)
- **WikiText-103 Perplexity**: 49.76 (Epoch 9)
- **Fusion Weights**: Semantic 61%, Concept 39%
- **Parameters**: 10% fewer than baseline (27M vs 30M)
- **Status**: âœ… Validated

### In Progress
- [ ] Baseline GPT comparison (rerunning with matched config)
- [ ] Kolosis + Temporal validation (architecture implemented)
- [ ] Multi-head ablation study

**Help wanted!** If you have GPU resources, we'd love help completing these experiments.

## ğŸ—ï¸ Architecture

Kolosis uses four specialized cognitive streams:

```
Input Text
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hierarchical Embeddings            â”‚
â”‚  Symbol â†’ Concept â†’ Law             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Symbol       â”‚ Concept      â”‚ Semantic     â”‚ Temporal     â”‚
â”‚ Stream       â”‚ Stream       â”‚ Stream       â”‚ Stream       â”‚
â”‚              â”‚              â”‚              â”‚              â”‚
â”‚ Pattern      â”‚ Abstraction  â”‚ Relations    â”‚ Memory       â”‚
â”‚ Recognition  â”‚ & Categories â”‚ & Context    â”‚ & History    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Learned Fusion (Softmax Weights)   â”‚
â”‚  Combines streams adaptively        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output Predictions
```

### Hierarchical Embeddings
```python
# Three levels of semantic understanding
Symbol:  "apple" â†’ [0.2, 0.5, ...]  # Surface pattern
Concept: "apple" â†’ fruit, food      # Abstract meaning  
Law:     "apple" â†’ gravity, organic # Universal rules
```

### Multi-Scale Temporal Attention
```python
Fast decay (Î³â‰ˆ0.7):   ~8 tokens    # Immediate context
Medium decay (Î³â‰ˆ0.9): ~100 tokens  # Sentence-level
Slow decay (Î³â‰ˆ0.98):  ~2000 tokens # Document-level
```

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/yourusername/kolosis.git
cd kolosis
pip install -r requirements.txt
```

### Training Kolosis V2 Minimal
```bash
python experiments/wikitext/train_kolosis_v2_minimal_single_head.py
```

### Training with Temporal Attention
```bash
python experiments/wikitext/train_kolosis_v2_temporal_single_head.py
```

### Using Pretrained Models
```python
from neural_networks.kolosis import KolosisV2MinimalSingleHead

# Load model
model = KolosisV2MinimalSingleHead(
    vocab_size=50257,
    n_embd=128,
    block_size=128,
    n_layer=4
)

# Load checkpoint
model.load_state_dict(torch.load('path/to/checkpoint.pt'))

# Generate text
output = model.generate(input_ids, max_new_tokens=100)

# Inspect cognitive balance
fusion = model.get_fusion_weight()
print(f"Concept: {fusion:.2%}, Semantic: {1-fusion:.2%}")
```

## ğŸ“Š Benchmarks

### WikiText-103 (Block Size: 128, Single Head)

| Model | Parameters | Perplexity | Status |
|-------|-----------|------------|--------|
| Kolosis V2 Minimal | 27.4M | **49.76** | âœ… Validated |
| Baseline GPT | 30.6M | *In Progress* | â³ Running |
| Kolosis + Temporal | 47.7M | *Pending* | ğŸ“‹ Planned |

*All experiments use non-overlapping windows and causal masking for fair comparison.*

## ğŸ§ª Reproducibility

All data leakage issues have been identified and fixed:
1. âœ… Non-overlapping training windows
2. âœ… Causal attention masks
3. âœ… Causal semantic embeddings

See [`docs/data_leakage_issues_and_fixes.md`](docs/data_leakage_issues_and_fixes.md) for details.

## ğŸ“š Documentation

- **[Kolosis Synopsis](docs/kolosis_synopsis.md)**: Comprehensive overview for general audience
- **[Data Leakage Fixes](docs/data_leakage_issues_and_fixes.md)**: Scientific rigor documentation
- **[Training Instructions](docs/temporal_attention_training_instructions.md)**: Detailed setup guide

## ğŸ“ Key Innovations

1. **Hierarchical Embeddings**: Symbol â†’ Concept â†’ Law (3-level semantic understanding)
2. **Cognitive Streams**: Explicit separation of thinking modes
3. **Learned Fusion**: Automatic cognitive balance discovery
4. **Multi-Scale Temporal**: Human-like memory with exponential decay

## ğŸ”¬ Research

### Comparison to Related Work

**Closest architectures:**
- **Mixture of Experts (MoE)**: Similar specialization, but Kolosis uses dense fusion vs sparse routing
- **Switch Transformer**: Similar expert routing, but Kolosis has cognitive specialization
- **o1 (OpenAI)**: Similar interpretability goals, but Kolosis has architectural transparency

**Key differences:**
- Kolosis: Cognitive-based specialization (Symbol/Concept/Semantic/Temporal)
- MoE: Pattern-based specialization (learned experts)
- Kolosis: Dense fusion (all streams active, weighted)
- MoE: Sparse routing (select subset of experts)

### Publications

*Coming soon - paper in preparation*

## ğŸ¤ Contributing

We welcome contributions! Areas where help is needed:

- **ğŸ–¥ï¸ GPU Resources**: Help run baseline and temporal experiments
- **ğŸ“Š Benchmarking**: Test on additional datasets (Penn TreeBank, Enwik8, etc.)
- **ğŸ”¬ Research**: Ablation studies, scaling experiments
- **ğŸ“ Documentation**: Tutorials, examples, use cases
- **ğŸ› Bug Reports**: Issues, edge cases, improvements

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- WikiText-103 dataset from Salesforce Research
- Inspired by human cognitive architecture research
- Built with PyTorch and Hugging Face Transformers

## ğŸ“ Contact

- **Issues**: [GitHub Issues](https://github.com/yourusername/kolosis/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/kolosis/discussions)
- **Email**: your.email@example.com

## ğŸ“– Citation

```bibtex
@software{kolosis2024,
  title={Kolosis: Cognitive Multi-Stream Architecture for Efficient Language Modeling},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/kolosis}
}
```

---

**Status**: Research Preview | **Version**: 0.1.0 | **Last Updated**: December 2024
